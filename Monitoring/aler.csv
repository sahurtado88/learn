Key,Value
deployment-status.b.yml,groups:
  - name: "deployment_status"
    rules:
      - alert: NoDeployment_systemmanager_ra-system_P2
        expr: |
          (kube_deployment_status_replicas{namespace="ra-system",deployment="systemmanager"} or on() vector(10000)) == 10000
        for: 5m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'In the cluster "{{ $externalLabels.cluster }}" systemmanager deployment does not exist in ra-system namespace'
          description: |
            'Please run the base-k8s-provision pipeline with the latest successful version in the cluster "{{ $externalLabels.cluster }}" , if you need more information to run the pipeline please contact the cheetah team.'
      - alert: NoDeployment_tenant-operator-controller-manager_ra-system_P2
        expr: |
          (kube_deployment_status_replicas{namespace="ra-system",deployment="tenant-operator-controller-manager"} or on() vector(10000)) == 10000
        for: 5m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'In the cluster "{{ $externalLabels.cluster }}" tenant-operator-controller-manager deployment does not exist in ra-system namespace'
          description: |
            'Please run the base-k8s-provision pipeline with the latest successful version in the cluster "{{ $externalLabels.cluster }}" , if you need more information to run the pipeline please contact the cheetah team.'
      - alert: NoDeployment_user-operator-controller-manager_ra-system_P2
        expr: |
          (kube_deployment_status_replicas{namespace="ra-system",deployment="user-operator-controller-manager"} or on() vector(10000)) == 10000
        for: 5m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'In the cluster "{{ $externalLabels.cluster }}" user-operator-controller-manager deployment does not exist in ra-system namespace'
          description: |
            'Please run the base-k8s-provision pipeline with the latest successful version in the cluster "{{ $externalLabels.cluster }}" , if you need more information to run the pipeline please contact the cheetah team.'

deployment-status.bc.yml,groups:
  - name: "deployment_status"
    rules:
      - alert: NoDeployment_policyagent_ra-system_P2
        expr: |
          (kube_deployment_status_replicas{namespace="ra-system",deployment="policyagent"} or on() vector(10000)) == 10000
        for: 5m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'In the cluster "{{ $externalLabels.cluster }}" policyagent deployment does not exist in ra-system namespace'
          description: |
            'Please validate that the deployment exists in cluster "{{ $externalLabels.cluster }}" , otherwise run the pipeline base-k8s-provision for base clusters or common-k8-provision for Common cluster (index 000) with the latest successful version, if you need more information to run the pipeline please contact the cheetah team'
      - alert: NoDeployment_policydirector_ra-system_P2
        expr: |
          (kube_deployment_status_replicas{namespace="ra-system",deployment="policydirector"} or on() vector(10000)) == 10000
        for: 5m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'In the cluster "{{ $externalLabels.cluster }}" policydirector deployment does not exist in ra-system namespace'
          description: |
            'Please validate that the deployment exists in cluster "{{ $externalLabels.cluster }}" , otherwise run the pipeline base-k8s-provision for base clusters or common-k8-provision for Common cluster (index 000) with the latest successful version, if you need more information to run the pipeline please contact the cheetah team'

deployment-status.bcg.yml,groups:
  - name: "deployment_status"
    rules:
      - alert: NoDeployment_istiod_istio_system_P2
        expr: |
          (kube_deployment_status_replicas{namespace="istio-system",deployment="istiod"} or on() vector(10000)) == 10000
        for: 5m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'In the cluster "{{ $externalLabels.cluster }}" istiod deployment does not exist in istio-system namespace'
          description: |
            'Please validate that the deployment exists, otherwise run the pipeline base-k8s-provision for base clusters or common-k8-provision for Common cluster (index 000) with the latest successful version, if the cluster is gitlab cluster please contact Craig telling the issue, if you need more information to run the pipeline please contact the cheetah team... For GITLAB DO NOT RUN ANY PIPELINE'

kubernetes-performance.bcg.yml,groups:
  - name: "KubernetesPerformance"
    rules:
      - alert: CPU_Usage_90_P3
        expr: |
          100 - (avg by (node) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
        for: 10m  
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'CPU Usage is reaching 90% in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'CPU is reaching 90% please check the Cluster and follow this process https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/3352494688/Restart+Key+Vault+Manager+Passive+POD+-+CPU+workaround'

      - alert: ETCD_DB_Total_Size_P3
        expr: |
          sum(apiserver_storage_size_bytes) > 3000000000
        for: 5m  
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'ETCD Database for the Kubernetes cluster "{{ $externalLabels.cluster }}" is reaching an abnormal size'
          description: |
            'Troubleshoot via kubectl and Azure to identify what is happening'

      - alert: CPU_Usage_80_P4
        expr: |
          100 - (avg by (node) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m  
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'CPU Usage is reaching 80% in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'CPU is reaching 80% please check the Cluster and follow this process https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/3352494688/Restart+Key+Vault+Manager+Passive+POD+-+CPU+workaround'

      - alert: Kubernetes_Secrets_P3
        expr: |
          sum(apiserver_storage_objects{resource="secrets"}) > 20000
        for: 1m  
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Number of secrets in the cluster "{{ $externalLabels.cluster }}" is reaching an abnormal size'
          description: |
            'validate the total number of secrets in the cluster running this command kubectl get secret -A and identify from where is coming this alert - the max number of secrets recommended by Microsoft is 33000'
          
      - alert: Pod_Restart_Repetitively_P2
        expr: |
          sum by (cluster, namespace, pod) (last_over_time(kube_pod_container_status_restarts_total{namespace=~"ra-system|istio-system|ra-common|ra-monitoring|gitlab|cert-manager"}[15m]) - last_over_time(kube_pod_container_status_restarts_total{namespace=~"ra-system|istio-system|ra-common|ra-monitoring|gitlab|cert-manager"}[15m] offset 15m)) > 3
        labels:
          service: na-Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'A Pod has been restarted frequently in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Check the pod logs to see what is the error related to the restarted pod, please check which one is the pod affected and validate if this one is critical for the FTDS functionality if so proceed to work on this asap, otherwise, change its priority to P3 and work on it'

loki.bcg.yml,# based on alert definitions from the loki chart, in loki/src/alerts.yaml
groups:
  - name: "loki"
    rules:
      - alert: LokiRequestErrors
        expr: |
          100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[2m])) by (pod, route)
            /
          sum(rate(loki_request_duration_seconds_count[2m])) by (pod, route)
            > 10
        for: 10m
        labels:
          service: cloudvertical
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          description: '{{ $externalLabels.cluster }} - {{ $labels.pod }} {{ $labels.route }} is experiencing {{ printf "%.2f" $value }}% errors.'
          summary: 'Loki high rate of request errors: {{ $labels.pod }}/{{ $labels.route }}'
      - alert: LokiRequestPanics
        expr: |
          sum(increase(loki_panic_total[10m])) by (pod) > 0
        labels:
          service: cloudvertical
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          description: '{{ $externalLabels.cluster }} - {{ $labels.pod }} is experiencing increase in panics of {{ printf "%.2f" $value }}'
          summary: 'Loki increase of panics: {{ $labels.pod }}'
      - alert: LokiRequestLatency
        expr: |
          histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[2m])) by (le, pod, route)) > 1
        for: 15m
        labels:
          service: cloudvertical
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          description: '{{ $externalLabels.cluster }} - {{ $labels.pod }} {{ $labels.route }} is experiencing {{ printf "%.2f" $value }}s 99th percentile latency.'
          summary: 'Loki high request latency: {{ $labels.pod }}/{{ $labels.route }}'
      - alert: LokiTooManyCompactorsRunning
        expr: |
          sum(loki_boltdb_shipper_compactor_running) by (pod) > 1
        for: 5m
        labels:
          service: cloudvertical
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          description: '{{ $externalLabels.cluster }} - {{ $labels.pod }} has had {{ printf "%.0f" $value }} compactors running for more than 5m. Only one compactor should run at a time.'
          summary: 'Loki too many compactors running: {{ $labels.pod }}'

pod-status.b.yml,groups:
  - name: "pod_status"
    rules:
      - alert: Pod_different_Running_P3
        expr: |
          sum(kube_pod_status_phase{phase=~"Failed|Unknown",exported_namespace!="kube-system", exported_namespace!="istio-system", exported_namespace!="ra-monitoring", exported_namespace!="ra-system"}) by (namespace)> 0
        for: 3m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Pod with status different Running in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Validate pod in diferent running status for more than 5 minutes'
      - alert: pod_pending_15_minutes_P3
        expr: |
          sum((sum(kube_pod_status_phase{phase="Pending"}) by (pod)) ==1 and sum((time()- kube_pod_created{})/ 3600) by (pod) > 0.5 )> 0
        for: 1m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'validate pod in pending status for more than 15 minutes in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Pod in status pending'
      

pod-status.bcg.yml,groups:
  - name: "pod_status"
    rules:
      - alert: Fail_istio_system_P3
        expr: |
          sum(kube_pod_status_phase{exported_namespace="istio-system",phase=~"Failed|Unknown"})> 0
        for: 5m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Fail or Unknown in the istio-system in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Pod in phase=Failed or Unknown in namespace istio-system'
      - alert: Fail_ra_system_P3
        expr: |
          sum(kube_pod_status_phase{exported_namespace="ra-system",phase=~"Failed|Unknown"})> 0
        for: 10m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'validate pods in ra-system namespace that are not in running state in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Pod in ra-system=Failed or Unknown in namespace'    
      - alert: Fail_ra_monitoring_P4
        expr: |
          sum(kube_pod_status_phase{phase=~"Failed|Unknown",exported_namespace="ra-monitoring"})> 0
        for: 10m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Validate pods in ra-monitoring namespace that are not in running state in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Pod ra-monitoring in status Failed or Unknown'
      - alert: Fail_kube_system_P3
        expr: |
          sum(kube_pod_status_phase{phase=~"Failed|Unknown",exported_namespace="kube-system"})>0
        for: 10m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Validate pods in kube-system namespace that are not in running state in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Pod kube-system in status Failed or Unknown'    
      - alert: Pod_failed_OOmKilled_P2
        expr: |
          kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} > 0
        for: 2m
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Validate pod {{ $labels.pod}} in {{ $labels.exported_namespaces }} namespace that is terminated by OOMKilled reason in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Pod terminated for reason OOMKilled'    

system-manager.b.yml,groups:
  - name: "SystemManager"
    rules:
      - alert: SystemManagerBrokenWorkspace_P2
        expr: |
          sum(broken_workspace_management_status{service_name="System Manager", namespace="ra-system"}) > 0
        for: 5m  
        labels:
          service: system-manager
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'System Manager Broken Workspace in cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Open this Confluence page https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/3844769528/System+Manager to get some workarounds,
            check the system manager logs, go to Grafana to get more info related to the alert, and open this GitHub page https://github.com/ra-ftds/ra-system-manager/blob/main/docs/SystemManager.md
            where some information can be found, otherwise, proceed to get help from the Phoenix team'

      - alert: SystemManagerInstallorUpgradeWorkspace_P2
        expr: |
          sum((requests_to_manage_workspace_number_of_failed{failed_request_type=~"The cluster contains a misconfigured Workspace that is not managed by Helm and thus cannot be (?:installed|upgraded)."} != 0
          unless requests_to_manage_workspace_number_of_failed{failed_request_type=~"The cluster contains a misconfigured Workspace that is not managed by Helm and thus cannot be (?:installed|upgraded)."} offset 15m) or
          (last_over_time(requests_to_manage_workspace_number_of_failed{failed_request_type=~"The cluster contains a misconfigured Workspace that is not managed by Helm and thus cannot be (?:installed|upgraded)."}[15m]) - 
          last_over_time(requests_to_manage_workspace_number_of_failed{failed_request_type=~"The cluster contains a misconfigured Workspace that is not managed by Helm and thus cannot be (?:installed|upgraded)."}[15m] offset 15m))) > 0
        for: 1m  
        labels:
          service: na-system-manager
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'System Manager Install or Upgrade Workspace in cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Open this Confluence page https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/3844769528/System+Manager to get some workarounds,
            check the system manager logs, go to Grafana to get more info related to the alert, and open this GitHub page https://github.com/ra-ftds/ra-system-manager/blob/main/docs/SystemManager.md
            where some information can be found, otherwise, proceed to get help from the Phoenix team'

      - alert: SystemManagerByCheetahUpgradeProcessFailed_P2
        expr: |
          sum((requests_to_manage_workspace_number_of_failed{failed_request_type!~"The cluster contains a misconfigured Workspace that is not managed by Helm and thus cannot be (?:installed|upgraded)."} != 0
          unless requests_to_manage_workspace_number_of_failed{failed_request_type!~"The cluster contains a misconfigured Workspace that is not managed by Helm and thus cannot be (?:installed|upgraded)."} offset 15m) or
          (last_over_time(requests_to_manage_workspace_number_of_failed{failed_request_type!~"The cluster contains a misconfigured Workspace that is not managed by Helm and thus cannot be (?:installed|upgraded)."}[15m]) -
          last_over_time(requests_to_manage_workspace_number_of_failed{failed_request_type!~"The cluster contains a misconfigured Workspace that is not managed by Helm and thus cannot be (?:installed|upgraded)."}[15m] offset 15m))) > 0
        for: 1m  
        labels:
          service: na-system-manager
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'System Manager By Cheetah Upgrade Process Failed'
          description: |
            'Open this Confluence page https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/3844769528/System+Manager to get some workarounds,
            check the system manager logs, go to Grafana to get more info related to the alert, and open this GitHub page https://github.com/ra-ftds/ra-system-manager/blob/main/docs/SystemManager.md 
            where some information can be found, otherwise, proceed to get help from the Phoenix team'

      - alert: SystemManagerWorkspaceDeploymentNotScaledDown_P2
        expr: |
          sum((requests_to_manage_workspace_number_of_failed{failed_request_type="Scale-down failed due to an internal error."}!= 0 
          unless requests_to_manage_workspace_number_of_failed{failed_request_type="Scale-down failed due to an internal error."} offset 15m) 
          or (last_over_time(requests_to_manage_workspace_number_of_failed{failed_request_type="Scale-down failed due to an internal error."}[15m]) 
          - last_over_time(requests_to_manage_workspace_number_of_failed{failed_request_type="Scale-down failed due to an internal error."}[15m] offset 15m)))  > 0
        for: 3m  
        labels:
          service: na-system-manager
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Workspace deployment did not scaled down and a new user can not be scaled up, because tier is taken.'
          description: |
            'Open this Confluence page https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/4055597170/System+Manager+Workspace+Deployment+Not+Scaled+Down to get some workarounds,
            check the system manager logs, go to Grafana to get more info related to the alert, and open this GitHub page https://github.com/ra-ftds/ra-system-manager/blob/main/docs/SystemManager.md
            where some information can be found, otherwise, proceed to get help from the Phoenix team'
      - alert: SystemManagerWorkspaceFailed_P2
        expr: |
          sum((requests_to_manage_workspace_number_of_failed{failed_request_type!="Scale-down failed due to an internal error."} != 0 
          unless requests_to_manage_workspace_number_of_failed{failed_request_type!="Scale-down failed due to an internal error."} offset 15m) 
          or (last_over_time(requests_to_manage_workspace_number_of_failed{failed_request_type!="Scale-down failed due to an internal error."}[15m]) 
          - last_over_time(requests_to_manage_workspace_number_of_failed{failed_request_type!="Scale-down failed due to an internal error."}[15m] offset 15m))) > 0
        for: 3m  
        labels:
          service: na-system-manager
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'User is not able to get access to FTDS services'
          description: |
            'Open this Confluence page https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/4055269574/System+Manager+Workspace+Failed to get some workarounds,
            check the system manager logs, go to Grafana to get more info related to the alert, and open this GitHub page https://github.com/ra-ftds/ra-system-manager/blob/main/docs/SystemManager.md
            where some information can be found, otherwise, proceed to get help from the Phoenix team'
      - alert: SystemManagerErroWithScalingPrerequisites_P2
        expr: |
          sum((requests_number_of_failed{failed_request_type!="No entitlement available to scale up the user."} != 0 
          unless requests_number_of_failed{failed_request_type!="No entitlement available to scale up the user."} offset 15m) 
          or (last_over_time(requests_number_of_failed{failed_request_type!="No entitlement available to scale up the user."}[15m]) 
          - last_over_time(requests_number_of_failed{failed_request_type!="No entitlement available to scale up the user."}[15m])))  > 0
        for: 3m  
        labels:
          service: na-system-manager
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Something is failing while evaluating scaling prerequisites'
          description: |
            'Open this Confluence page https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/4054909093/System+Manager+Error+With+Scaling+Prerequisite to get some workarounds,
            check the system manager logs, go to Grafana to get more info related to the alert, and open this GitHub page https://github.com/ra-ftds/ra-system-manager/blob/main/docs/SystemManager.md
            where some information can be found, otherwise, proceed to get help from the Phoenix team '

tenant-lostpvc.b.yml,groups:
  - name: "TenantLostPVC"
    rules:
      - alert: Tenant_Lost_PVC_P2
        expr: |
          sum by (persistentvolumeclaim,namespace,phase) (last_over_time(kube_persistentvolumeclaim_status_phase{phase="Lost", namespace!="ra-system|aks-command|cert-manager|default|gatekeeper-system|harness-system|istio-system|kube-node-lease|kube-public|kube-system|ra-monitoring"}[15m])) > 0
        for: 5m
        labels:
          service: kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'User having issues... A PVC in the Cluster "{{ $externalLabels.cluster }}" is not working properly, therefore, the user workspace deployment will not spin up'
          description: |
            'As a workaround to this issue follow this documentation https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/3808100507/Lost+PVC'

tenant-operator.b.yml,groups:
  - name: "TenantOperator"
    rules:
      - alert: Fail_Tenant_Operator_P2
        expr: |
          sum(broken_operator_status{service_name="Tenant Operator"}) > 0
        for: 2m
        labels:
          service: tenant-operator
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Tenant Operator is not working properly in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Please validate the tenant operator in the cluster "{{ $externalLabels.cluster }}"- Check Grafana - validate the message in this github page 
            https://github.com/ra-ftds/ra-tenant-operator/blob/main/docs/TenantOperatorDesignDocument.md#predefined-messages-for-warnings-and-errors-to-be-stored-in-the-status-object'

user-operator.b.yml,groups:
  - name: "UserOperator"
    rules:
      - alert: Fail_User_Operator_P2
        expr: |
          sum(users_number_of_broken{pod=~"user-operator-controller-manager.+",service_name="User Operator"}) > 0 
        for: 2m
        labels:
          service: user-operator
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'User Operator is not working properly in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Please validate the user operator in the cluster "{{ $externalLabels.cluster }}" - check Grafana - validate the message in this github page 
            https://github.com/ra-ftds/ra-user-operator/blob/main/docs/ra-user-operator-DD.md#predefined-messages-for-errors-to-be-stored-in-the-status-object'

windows-kubernetesperformance.b.yml,groups:
  - name: "WindowsKubernetesPerformance"
    rules:
      - alert: Windows_CPU_Usage_90_P3
        expr: |
          sum(rate(windows_container_cpu_usage_seconds_total[2m])) by (instance)>90
        for: 15m  
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Windows node - CPU Usage is reaching 90% in the cluster "{{ $externalLabels.cluster }}"'
          description: |
            'Connect to the cluster and run kubectl top nodes to identify which node is running out of CPU.'

      - alert: Windows_Memory_Usage_P3
        expr: |
          windows_memory_available_bytes /  1024^3 < 5
        for: 15m  
        labels:
          service: Kubernetes
          cluster: '{{ $externalLabels.cluster }}'
        annotations:
          summary: 'Windows node - Memory Usage is reaching 90% in the cluster "{{ $externalLabels.cluster }}'
          description: |
            'Connect to the cluster and run kubectl top nodes to identify which node is running out of memory'

