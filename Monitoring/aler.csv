apiVersion: v1
data:
  deployment-status.b.yml: |
    groups:
      - name: "deployment_status"
        rules:
          - alert: NoDeployment_systemmanager_ra-system_P2
            expr: |
              (kube_deployment_status_replicas{namespace="ra-system",deployment="systemmanager"} or on() vector(10000)) == 10000
            for: 5m
            labels:
              service: Kubernetes
              cluster: '{{ $externalLabels.cluster }}'
            annotations:
              summary: 'In the cluster "{{ $externalLabels.cluster }}" systemmanager deployment does not exist in ra-system namespace'
              description: |
                'Please run the base-k8s-provision pipeline with the latest successful version in the cluster "{{ $externalLabels.cluster }}" , if you need more information to run the pipeline please contact the cheetah team.'
          - alert: NoDeployment_tenant-operator-controller-manager_ra-system_P2
            expr: |
              (kube_deployment_status_replicas{namespace="ra-system",deployment="tenant-operator-controller-manager"} or on() vector(10000)) == 10000
            for: 5m
            labels:
              service: Kubernetes
              cluster: '{{ $externalLabels.cluster }}'
            annotations:
              summary: 'In the cluster "{{ $externalLabels.cluster }}" tenant-operator-controller-manager deployment does not exist in ra-system namespace'
              description: |
                'Please run the base-k8s-provision pipeline with the latest successful version in the cluster "{{ $externalLabels.cluster }}" , if you need more information to run the pipeline please contact the cheetah team.'
          - alert: NoDeployment_user-operator-controller-manager_ra-system_P2
            expr: |
              (kube_deployment_status_replicas{namespace="ra-system",deployment="user-operator-controller-manager"} or on() vector(10000)) == 10000
            for: 5m
            labels:
              service: Kubernetes
              cluster: '{{ $externalLabels.cluster }}'
            annotations:
              summary: 'In the cluster "{{ $externalLabels.cluster }}" user-operator-controller-manager deployment does not exist in ra-system namespace'
              description: |
                'Please run the base-k8s-provision pipeline with the latest successful version in the cluster "{{ $externalLabels.cluster }}" , if you need more information to run the pipeline please contact the cheetah team.'
  deployment-status.bc.yml: |
    groups:
      - name: "deployment_status"
        rules:
          - alert: NoDeployment_policyagent_ra-system_P2
            expr: |
              (kube_deployment_status_replicas{namespace="ra-system",deployment="policyagent"} or on() vector(10000)) == 10000
            for: 5m
            labels:
              service: Kubernetes
              cluster: '{{ $externalLabels.cluster }}'
            annotations:
              summary: 'In the cluster "{{ $externalLabels.cluster }}" policyagent deployment does not exist in ra-system namespace'
              description: |
                'Please validate that the deployment exists in cluster "{{ $externalLabels.cluster }}" , otherwise run the pipeline base-k8s-provision for base clusters or common-k8-provision for Common cluster (index 000) with the latest successful version, if you need more information to run the pipeline please contact the cheetah team'
          - alert: NoDeployment_policydirector_ra-system_P2
            expr: |
              (kube_deployment_status_replicas{namespace="ra-system",deployment="policydirector"} or on() vector(10000)) == 10000
            for: 5m
            labels:
              service: Kubernetes
              cluster: '{{ $externalLabels.cluster }}'
            annotations:
              summary: 'In the cluster "{{ $externalLabels.cluster }}" policydirector deployment does not exist in ra-system namespace'
              description: |
                'Please validate that the deployment exists in cluster "{{ $externalLabels.cluster }}" , otherwise run the pipeline base-k8s-provision for base clusters or common-k8-provision for Common cluster (index 000) with the latest successful version, if you need more information to run the pipeline please contact the cheetah team'
  deployment-status.bcg.yml: |
    groups:
      - name: "deployment_status"
        rules:
          - alert: NoDeployment_istiod_istio_system_P2
            expr: |
              (kube_deployment_status_replicas{namespace="istio-system",deployment="istiod"} or on() vector(10000)) == 10000
            for: 5m
            labels:
              service: Kubernetes
              cluster: '{{ $externalLabels.cluster }}'
            annotations:
              summary: 'In the cluster "{{ $externalLabels.cluster }}" istiod deployment does not exist in istio-system namespace'
              description: |
                'Please validate that the deployment exists, otherwise run the pipeline base-k8s-provision for base clusters or common-k8-provision for Common cluster (index 000) with the latest successful version, if the cluster is gitlab cluster please contact Craig telling the issue, if you need more information to run the pipeline please contact the cheetah team... For GITLAB DO NOT RUN ANY PIPELINE'
  kubernetes-performance.bcg.yml: "groups:\n  - name: \"KubernetesPerformance\"\n
    \   rules:\n      - alert: CPU_Usage_90_P3\n        expr: |\n          100 - (avg
    by (node) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 90\n        for:
    10m  \n        labels:\n          service: Kubernetes\n          cluster: '{{
    $externalLabels.cluster }}'\n        annotations:\n          summary: 'CPU Usage
    is reaching 90% in the cluster \"{{ $externalLabels.cluster }}\"'\n          description:
    |\n            'CPU is reaching 90% please check the Cluster and follow this process
    https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/3352494688/Restart+Key+Vault+Manager+Passive+POD+-+CPU+workaround'\n\n
    \     - alert: ETCD_DB_Total_Size_P3\n        expr: |\n          sum(apiserver_storage_size_bytes)
    > 3000000000\n        for: 5m  \n        labels:\n          service: Kubernetes\n
    \         cluster: '{{ $externalLabels.cluster }}'\n        annotations:\n          summary:
    'ETCD Database for the Kubernetes cluster \"{{ $externalLabels.cluster }}\" is
    reaching an abnormal size'\n          description: |\n            'Troubleshoot
    via kubectl and Azure to identify what is happening'\n\n      - alert: CPU_Usage_80_P4\n
    \       expr: |\n          100 - (avg by (node) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m]))
    * 100) > 80\n        for: 10m  \n        labels:\n          service: Kubernetes\n
    \         cluster: '{{ $externalLabels.cluster }}'\n        annotations:\n          summary:
    'CPU Usage is reaching 80% in the cluster \"{{ $externalLabels.cluster }}\"'\n
    \         description: |\n            'CPU is reaching 80% please check the Cluster
    and follow this process https://rockwellautomation.atlassian.net/wiki/spaces/CS/pages/3352494688/Restart+Key+Vault+Manager+Passive+POD+-+CPU+workaround'\n\n
    \     - alert: Kubernetes_Secrets_P3\n        expr: |\n          sum(apiserver_storage_objects{resource=\"secrets\"})
    > 20000\n        for: 1m  \n        labels:\n          service: Kubernetes\n          cluster:
    '{{ $externalLabels.cluster }}'\n        annotations:\n          summary: 'Number
    of secrets in the cluster \"{{ $externalLabels.cluster }}\" is reaching an abnormal
    size'\n          description: |\n            'validate the total number of secrets
    in the cluster running this command kubectl get secret -A and identify from where
    is coming this alert - the max number of secrets recommended by Microsoft is 33000'\n
    \         \n      - alert: Pod_Restart_Repetitively_P2\n        expr: |\n          sum
    by (cluster, namespace, pod) (last_over_time(kube_pod_container_status_restarts_total{namespace=~\"ra-system|istio-system|ra-common|ra-monitoring|gitlab|cert-manager\"}[15m])
    - last_over_time(kube_pod_container_status_restarts_total{namespace=~\"ra-system|istio-system|ra-common|ra-monitoring|gitlab|cert-manager\"}[15m]
    offset 15m)) > 3\n        labels:\n          service: na-Kubernetes\n          cluster:
    '{{ $externalLabels.cluster }}'\n        annotations:\n          summary: 'A Pod
    has been restarted frequently in the cluster \"{{ $externalLabels.cluster }}\"'\n
    \         description: |\n            'Check the pod logs to see what is the error
    related to the restarted pod, please check which one is the pod affected and validate
    if this one is critical for the FTDS functionality if so proceed to work on this
    asap, otherwise, change its priority to P3 and work on it'\n"
  loki.bcg.yml: |
    # based on alert definitions from the loki chart, in loki/src/alerts.yaml
    groups:
      - name: "loki"
        rules:
          - alert: LokiRequestErrors
            expr: |
              100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[2m])) by (pod, route)
                /
              sum(rate(loki_request_duration_seconds_count[2m])) by (pod, route)
                > 10
            for: 10m
            labels:
              service: cloudvertical
              cluster: '{{ $externalLabels.cluster }}'
            annotations:
              description: '{{ $externalLabels.cluster }} - {{ $labels.pod }} {{ $labels.route }} is experiencing {{ printf "%.2f" $value }}% errors.'
              summary: 'Loki high rate of request errors: {{ $labels.pod }}/{{ $labels.route }}'
          - alert: LokiRequestPanics
            expr: |
              sum(increase(loki_panic_total[10m])) by (pod) > 0
            labels:
              service: cloudvertical
              cluster: '{{ $externalLabels.cluster }}'
            annotations:
              description: '{{ $externalLabels.cluster }} - {{ $labels.pod }} is experiencing increase in panics of {{ printf "%.2f" $value }}'
              summary: 'Loki increase of panics: {{ $labels.pod }}'
          - alert: LokiRequestLatency
            expr: |
              histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[2m])) by (le, pod, route)) > 1
            for: 15m
            labels:
              service: cloudvertical
              cluster: '{{ $externalLabels.cluster }}'
            annotations:
              description: '{{ $externalLabels.cluster }} - {{ $labels.pod }} {{ $labels.route }} is experiencing {{ printf "%.2f" $value }}s 99th percentile latency.'
              summary: 'Loki high request latency: {{ $labels.pod }}/{{ $labels.route }}'
          - alert: LokiTooManyCompactorsRunning
            expr: |
              sum(loki_boltdb_shipper_compactor_running) by (pod) > 1
            for: 5m
            labels:
              service: cloudvertical
              cluster: '{{ $externalLabels.cluster }}'
            annotations:
              description: '{{ $externalLabels.cluster }} - {{ $labels.pod }} has had {{ printf "%.0f" $value }} compactors running for more than 5m. Only one compactor should run at a time.'
              summary: 'Loki too many compactors running: {{ $labels.pod }}'
  pod-status.b.yml: "groups:\n  - name: \"pod_status\"\n    rules:\n      - alert:
    Pod_different_Running_P3\n        expr: |\n          sum(kube_pod_status_phase{phase=~\"Failed|Unknown\",exported_namespace!=\"kube-system\",
    exported_namespace!=\"istio-system\", exported_namespace!=\"ra-monitoring\", exported_namespace!=\"ra-system\"})
    by (namespace)> 0\n        for: 3m\n        labels:\n          service: Kubernetes\n
    \         cluster: '{{ $externalLabels.cluster }}'\n        annotations:\n          summary:
    'Pod with status different Running in the cluster \"{{ $externalLabels.cluster
    }}\"'\n          description: |\n            'Validate pod in diferent running
    status for more than 5 minutes'\n      - alert: pod_pending_15_minutes_P3\n        expr:
    |\n          sum((sum(kube_pod_status_phase{phase=\"Pending\"}) by (pod)) ==1
    and sum((time()- kube_pod_created{})/ 3600) by (pod) > 0.5 )> 0\n        for:
    1m\n        labels:\n          service: Kubernetes\n          cluster: '{{ $externalLabels.cluster
    }}'\n        annotations:\n          summary: 'validate pod in pending status
    for more than 15 minutes in the cluster \"{{ $externalLabels.cluster }}\"'\n          description:
    |\n            'Pod in status pending'\n      \n"
  pod-status.bcg.yml: "groups:\n  - name: \"pod_status\"\n    rules:\n      - alert:
    Fail_istio_system_P3\n        expr: |\n          sum(kube_pod_status_phase{exported_namespace=\"istio-system\",phase=~\"Failed|Unknown\"})>
    0\n        for: 5m\n        labels:\n          service: Kubernetes\n          cluster:
    '{{ $externalLabels.cluster }}'\n        annotations:\n          summary: 'Fail
    or Unknown in the istio-system in the cluster \"{{ $externalLabels.cluster }}\"'\n
    \         description: |\n            'Pod in phase=Failed or Unknown in namespace
    istio-system'\n      - alert: Fail_ra_system_P3\n        expr: |\n          sum(kube_pod_status_phase{exported_namespace=\"ra-system\",phase=~\"Failed|Unknown\"})>
    0\n        for: 10m\n        labels:\n          service: Kubernetes\n          cluster:
    '{{ $externalLabels.cluster }}'\n        annotations:\n          summary: 'validate
    pods in ra-system namespace that are not in running state in the cluster \"{{
    $externalLabels.cluster }}\"'\n          description: |\n            'Pod in ra-system=Failed
    or Unknown in namespace'    \n      - alert: Fail_ra_monitoring_P4\n        expr:
    |\n          sum(kube_pod_status_phase{phase=~\"Failed|Unknown\",exported_namespace=\"ra-monitoring\"})>
    0\n        for: 10m\n        labels:\n          service: Kubernetes\n          cluster:
    '{{ $externalLabels.cluster }}'\n        annotations:\n          summary: 'Validate
    pods in ra-monitoring namespace that are not in running state in the cluster \"{{
    $externalLabels.cluster }}\"'\n          description: |\n            'Pod ra-monitoring
    in status Failed or Unknown'\n      - alert: Fail_kube_system_P3\n        expr:
    |\n          sum(kube_pod_status_phase{phase=~\"Failed|Unknown\",exported_namespace=\"kube-system\"})>0\n
    \       for: 10m\n        labels:\n          service: Kubernetes\n          cluster:
    '{{ $externalLabels.cluster }}'\n        annotations:\n          summary: 'Validate
    pods in kube-system namespace that are not in running state in the cluster \"{{
    $externalLabels.cluster }}\"'\n          description: |\n            'Pod kube-system
    in status Failed or Unknown'    \n      - alert: Pod_failed_OOmKilled_P2\n        expr:
    |\n          kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}
    > 0\n        for: 2m\n        labels:\n          service: Kubernetes\n          cluster:
    '{{ $externalLabels.cluster }}'\n        annotations:\n          summary: 'Validate
    pod {{ $labels.pod}} in {{ $labels.exported_namespaces }} namespace that is terminated
    by OOMKilled reason in the cluster \"{{ $externalLabels.cluster }}\"'\n          description:
    |\n            'Pod terminated for reason OOMKilled'    \n"